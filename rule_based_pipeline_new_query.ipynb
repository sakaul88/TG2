{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cac5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state=1111\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import copy\n",
    "import heapq\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "# import imblearn\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97d3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(client_info,tranx):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    client_info.columns= client_info.columns.str.lower()\n",
    "    tranx.columns= tranx.columns.str.lower()\n",
    "    \n",
    "    cwd= os.getcwd()\n",
    "    cons_df= pd.merge(tranx,client_info, how='inner',on=['cont_id'])\n",
    "    \n",
    "    input_df= copy.deepcopy(cons_df)\n",
    "    \n",
    "    cons_df.drop(['aureq_env_a_id_id','aureq_env_cpl_pan','aureq_tx_mrchntctgyc',\\\n",
    "                  'authorresult_rspnt','filler1','tx_id'], axis=1, inplace= True)\n",
    "    \n",
    "    col_num= cons_df.columns.get_loc('annual_invest')\n",
    "    cons_df.loc[cons_df['annual_invest']<=0,'annual_invest']=1\n",
    "    cons_df['log_invest']= np.log(cons_df['annual_invest'])\n",
    "    cons_df['log_income']= np.log(cons_df['annual_income'])\n",
    "    cons_df['log_ttlamt']= np.log(cons_df['aureq_tx_dt_ttlamt'])\n",
    "    \n",
    "    cons_df.drop(['aureq_tx_dt_ttlamt','annual_invest','annual_income'],axis=1,inplace=True)\n",
    "    \n",
    "    cons_df['log_ageyears']= np.log(cons_df['age_years'])\n",
    "    \n",
    "    cons_df.drop(['age_years','age'],axis=1,inplace=True)\n",
    "    \n",
    "    year=[]\n",
    "    month=[]\n",
    "    for i in tqdm(range(cons_df.shape[0])):\n",
    "        a= cons_df.loc[i,'hdr_credtt'].split('-')\n",
    "        year.append(a[0])\n",
    "        month.append(a[1])\n",
    "        \n",
    "    cons_df['month']= month\n",
    "    cons_df['month']= cons_df['month'].str.replace(r'^(0+)', '')\n",
    "    cons_df['year']= year\n",
    "    cons_df= cons_df[cons_df['year']=='2013']\n",
    "    \n",
    "    cons_df['time']= cons_df['hdr_credtt'].apply(lambda x: x.split('T')[1])\n",
    "    cons_df['date']= cons_df['hdr_credtt'].apply(lambda x: x.split('T')[0])\n",
    "    cons_df['day']= cons_df['date'].apply(lambda x: x.split('-')[2])\n",
    "    cons_df['hour']= cons_df['time'].apply(lambda x: x.split(':')[0])\n",
    "    cons_df['minute']= cons_df['time'].apply(lambda x: x.split(':')[1])\n",
    "\n",
    "    cons_df.drop(['hdr_credtt','time','year','date'],axis=1, inplace=True)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_count_mid_id.pkl','rb') as cnt_mid:\n",
    "        count_mid_id= pickle.load(cnt_mid)\n",
    "\n",
    "    with open(cwd + '\\\\' + 'query_rb_count_cmonnm.pkl','rb') as cnt_cmon:\n",
    "        count_cmonnm= pickle.load(cnt_cmon)\n",
    "\n",
    "    with open(cwd + '\\\\' + 'query_rb_count_postc.pkl','rb') as cnt_pst:\n",
    "        count_postc= pickle.load(cnt_pst)\n",
    "\n",
    "    with open(cwd + '\\\\' + 'query_rb_count_contid.pkl','rb') as cnt_cid:\n",
    "        count_contid= pickle.load(cnt_cid)\n",
    "    \n",
    "    cons_df['month']= cons_df['month'].astype(str)\n",
    "    cons_df['gender']= cons_df['gender'].map({0:'g0',1:'g1'})\n",
    "    \n",
    "    cons_df['aureq_env_m_id_id']= cons_df['aureq_env_m_id_id'].map(count_mid_id)\n",
    "    cons_df['aureq_env_m_cmonnm']= cons_df['aureq_env_m_cmonnm'].map(count_cmonnm)\n",
    "    cons_df['mdm_postal_code_id']= cons_df['mdm_postal_code_id'].map(count_postc)\n",
    "    cons_df['cont_id']= cons_df['cont_id'].map(count_contid)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_card_enc.pkl','rb') as c_enc:\n",
    "        card_enc= pickle.load(c_enc)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_month_enc.pkl','rb') as m_enc:\n",
    "        month_enc= pickle.load(m_enc)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_gen_enc.pkl','rb') as g_enc:\n",
    "        gen_enc= pickle.load(g_enc)\n",
    "    \n",
    "    cdums= card_enc.transform(cons_df['aureq_env_c_cardbrnd'].values.reshape(-1,1)).toarray()\n",
    "    card_dum= pd.DataFrame(cdums,columns= list(card_enc.categories_[0]))\n",
    "    card_dum.columns= card_dum.columns.str.lower()\n",
    "\n",
    "    mdums= month_enc.transform(cons_df['month'].values.reshape(-1,1)).toarray()\n",
    "    month_dum= pd.DataFrame(mdums,columns= list(month_enc.categories_[0]))\n",
    "    month_dum.columns= ['month_1','month_2','month_3','month_4','month_5','month_6','month_7','month_8', 'month_9','month_10','month_11','month_12']\n",
    "    \n",
    "    gdums= gen_enc.transform(cons_df['gender'].values.reshape(-1,1)).toarray()\n",
    "    gen_dum= pd.DataFrame(gdums,columns= list(gen_enc.categories_[0]))\n",
    "    gen_dum.columns= ['g0','g1']\n",
    "    \n",
    "    cons_df= pd.concat([cons_df,card_dum,month_dum,gen_dum],axis=1)\n",
    "    cons_df.drop(['aureq_env_c_cardbrnd','month','gender','g0'],axis=1, inplace=True)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_std_scl.pkl','rb') as std_scl:\n",
    "        std_scaler= pickle.load(std_scl)\n",
    "    \n",
    "    consdf_scl= pd.DataFrame(std_scaler.transform(cons_df))\n",
    "    consdf_scl.columns= cons_df.columns\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_to_del.pkl','rb') as vi_del:\n",
    "        to_del= pickle.load(vi_del)\n",
    "    \n",
    "    consdf_scl.drop(to_del,axis=1,inplace=True)\n",
    "        \n",
    "    cons_zscr_df= pd.DataFrame()\n",
    "    col_names= consdf_scl.columns\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_dict_params.pkl','rb') as par_dict:\n",
    "        dict_params= pickle.load(par_dict)\n",
    "    \n",
    "    for col in col_names:\n",
    "        zval=[]\n",
    "        mean= dict_params[col][0]\n",
    "        std= dict_params[col][1]\n",
    "        z_min= dict_params[col][2]\n",
    "        z_max= dict_params[col][3]\n",
    "    \n",
    "        for i in range(consdf_scl.shape[0]):\n",
    "            zval.append(np.abs((consdf_scl[col][i]-mean)/std))\n",
    "        zval= np.array(zval)\n",
    "        cons_zscr_df[col]= (zval-z_min)/(z_max-z_min)\n",
    "    \n",
    "    cons_zscr_df= cons_zscr_df.add_prefix('zscore_')\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_del_cols.pkl','rb') as dcols:\n",
    "        del_cols= pickle.load(dcols)\n",
    "    \n",
    "    cons_zscr_df.drop(del_cols,axis=1,inplace=True)\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_allcols_zscr.pkl','rb') as tr_allcols:\n",
    "        tr_allcols_zscr= pickle.load(tr_allcols)\n",
    "    \n",
    "    allcols_zscr = cons_zscr_df.mean(axis=1)\n",
    "    cons_zscr_df[\"all_cols_zscore\"] = (allcols_zscr - np.min(tr_allcols_zscr))/(np.max(tr_allcols_zscr) - np.min(tr_allcols_zscr))\n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_frd_mean.pkl','rb') as fmean:\n",
    "        frd_mean= pickle.load(fmean)\n",
    "\n",
    "    with open(cwd + '\\\\' + 'query_rb_frd_std.pkl','rb') as fstd:\n",
    "        frd_std= pickle.load(fstd)    \n",
    "    \n",
    "    with open(cwd + '\\\\' + 'query_rb_nonfrd_mean.pkl','rb') as nfmean:\n",
    "        nonfrd_mean= pickle.load(nfmean)\n",
    "\n",
    "    with open(cwd + '\\\\' + 'query_rb_nonfrd_std.pkl','rb') as nfstd:\n",
    "        nonfrd_std= pickle.load(nfstd)\n",
    "    \n",
    "    frd_ll= (frd_mean)-(1.85*frd_std)\n",
    "    frd_ul= (frd_mean)+(1.85*frd_std)\n",
    "    \n",
    "    cons_zscr_df['dist_frd_mean']= cons_zscr_df['all_cols_zscore']-frd_mean\n",
    "    cons_zscr_df['dist_nonfrd_mean']= cons_zscr_df['all_cols_zscore']-nonfrd_mean\n",
    "\n",
    "    cons_zscr_df['dist_frd_mean']= cons_zscr_df['dist_frd_mean'].abs() \n",
    "    cons_zscr_df['dist_nonfrd_mean']= cons_zscr_df['dist_nonfrd_mean'].abs()\n",
    "\n",
    "    cons_zscr_df['diff']= cons_zscr_df['dist_frd_mean']-cons_zscr_df['dist_nonfrd_mean']\n",
    "    \n",
    "    pos_index= cons_zscr_df[(cons_zscr_df['all_cols_zscore']>frd_ll) & (cons_zscr_df['all_cols_zscore']<frd_ul)].index\n",
    "    \n",
    "    input_df['predict']='non_fraud'\n",
    "    \n",
    "    input_df.loc[pos_index,'predict']= 'fraud'\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration= end_time - start_time\n",
    "    \n",
    "    fraud_tranx= input_df[input_df['predict']=='fraud']\n",
    "    \n",
    "    return fraud_tranx, input_df, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99359b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_client_info= pd.read_csv('C:\\\\Users\\\\satyaav\\\\credit_card\\\\query_client_info.csv')\n",
    "# query_tranx= pd.read_csv('C:\\\\Users\\\\satyaav\\\\credit_card\\\\query_tranx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a49994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_tranx_frac= query_tranx.sample(frac=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2c74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 973244/973244 [00:13<00:00, 74262.24it/s]\n"
     ]
    }
   ],
   "source": [
    "query_fraud,query_all_tranx,query_duration= test_pipeline(query_client_info,query_tranx_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2ef9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46886, 22)\n",
      "(973244, 22)\n",
      "0:02:35.118333\n"
     ]
    }
   ],
   "source": [
    "print(query_fraud.shape)\n",
    "print(query_all_tranx.shape)\n",
    "print(query_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e504f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13439626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
